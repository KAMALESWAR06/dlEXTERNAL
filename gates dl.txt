w1 = 0.4
w2 = 1.7
b = 0
n = 0.2
inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
desired_output = [0,1,1,1]

def activation(w1, w2, x1, x2, b):
    x = x1 * w1 + x2 * w2 + b
    return 1 if x >= 1 else 0

def perceptron():
    global w1, w2, b
    
    for epoch in range(100):
        total_error = 0
        for i in range(len(inputs)):
            x1, x2 = inputs[i]
            output = activation(w1, w2, x1, x2, b)
            error = desired_output[i] - output
            
            if error != 0:
                w1 += error * n * x1
                w2 += error * n * x2
                b += error * n
                total_error += abs(error)
        if total_error == 0:
            break

perceptron()
print("Updated weights are:", w1, w2)
for i in range(len(inputs)):
    x1, x2 = inputs[i]
    output = activation(w1, w2, x1, x2, b)
    print(f"Input: ({x1}, {x2})  Output: {output}")












import numpy as np
 
def unitStep(v):
    if v >= 0:
        return 1
    else:
        return 0
 
def perceptronModel(x, w, b):
    v = np.dot(w, x) + b
    y = unitStep(v)
    return y
 
def NOT_logicFunction(x):
    wNOT = -1
    bNOT = 0.5
    return perceptronModel(x, wNOT, bNOT)
 
def AND_logicFunction(x):
    w = np.array([1, 1])
    bAND = -1.5
    return perceptronModel(x, w, bAND)
 
def OR_logicFunction(x):
    w = np.array([1, 1])
    bOR = -0.5
    return perceptronModel(x, w, bOR)
 
def XOR_logicFunction(x):
    y1 = AND_logicFunction(x)
    y2 = OR_logicFunction(x)
    y3 = NOT_logicFunction(y1)
    final_x = np.array([y2, y3])
    finalOutput = AND_logicFunction(final_x)
    return finalOutput
 
test1 = np.array([0, 1])
test2 = np.array([1, 1])
test3 = np.array([0, 0])
test4 = np.array([1, 0])
 
print("XOR({}, {}) = {}".format(0, 1, XOR_logicFunction(test1)))
print("XOR({}, {}) = {}".format(1, 1, XOR_logicFunction(test2)))
print("XOR({}, {}) = {}".format(0, 0, XOR_logicFunction(test3)))
print("XOR({}, {}) = {}".format(1, 0, XOR_logicFunction(test4)))












w1= 0
b=0
desired_outputs = [1,0]
n = 0.1

def activate(x):
    return 1 if x >= 0 else 0

def perceptron(inputs):
    global w1,b
    for epoch in range(100):
        total_error = 0
        for i in range(len(inputs)):
            A = inputs[i]
            output = activate(w1 * A + b)
            error = desired_outputs[i]- output
            
            w1 += n * error * A
            b += n* error
            total_error += abs(error)
        if total_error == 0:
            return w1,b



inputs = [(0), (1)]
w1,b=perceptron(inputs)
for i in range(len(inputs)):
    A = inputs[i]
    output = activate(w1 * A  + b)
    print(f"Input: ({A})  Output: {output}")